\documentclass[a4paper, 11pt]{article}
%% Math Support
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumitem}
%independent sign
\usepackage{rotating}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
%% antifloat image
\usepackage{float}
%[H] for fix 
%[htbp] for float
\usepackage{authblk}
\usepackage{cite}
%% Graphics Support
\usepackage{listings}%begin MATLAB 
\usepackage{color}
\usepackage{placeins}
\usepackage{lipsum}

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}
\setcounter{MaxMatrixCols}{21}
\footnotesize
\setlength{\arraycolsep}{2.5pt}
\medmuskip = 1mu

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
	language=MATLAB,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3
}%end MATLAB 
% lstinputlisting

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{sincom.png}
%   \caption{Comparing Polynomial and Taylor Approximation with Sine function}
%    \label{figure:sincom}
%   \end{figure}
% \FloatBarrier

\setlength\parindent{0pt}
\usepackage{color}
\usepackage{xcolor}
\usepackage{graphicx}  % png/jpg support
\usepackage{fullpage} % changes the margin
\title{Project Summary: Document Classifier}
\author[1]{Arjun Patel}
\author[1]{Harshil Prajapati}
\author[1]{Bowen Song}
\affil[1]{Department of Electrical and Computer Engineering, Boston University}
% \author{Arjun Patel\\
% Boston University\\
% 8 St. Mary's st,\\
% {\tt\small adpatel@bu.edu}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Harshil Prajapati\\
% {\tt\small harshil@bu.edu}
% \and
% Bowen Song\\
% {\tt\small sbowen@bu.edu}
% }
\begin{document}
	\maketitle
	\section{Problem Definition}
	This project explores machine learning algorithms for document classification. The specific application of the project is sensitive to datasets. The contribution of the project is to compare and contrast algorithm performances based on results and computation efficiency with respect to different types of datasets.
	
	\section{Literature Review}
	Document Classification is a traditional problem where usually a bag-of-words approach is to extract features and are used for supervised classification using Naive Bayes or Support Vector Machine (SVM) algorithms \cite{sachan2018investigating}. 
	
	\hspace{15pt} In classical papers, the order of words is ignored, but the order can be included by using Term Frequency - Inverse Document Frequency (TF-IDF) they can be incorporated \cite{maes1995agents}. Latent semantic indexing was introduced in \cite{deerwester1990indexing} which uses Singular Value Decomposition onto TF-IDF space. This was modified by \cite{hofmann1999probabilistic} to fitting the model to data using maximum likelihood or Bayesian methods known as Probabilistic latent semantic indexing (pLSI). Latent Dirichlet allocation (LDA) uses mixture models considering the exchangeability of both words and documents \cite{blei2003latent}. Kernel methods (KMs) are an effective alternative to explicit feature extraction. Another way to efficiently extract features is based on finding conditional probability using n-gram models \cite{furnkranz1998study} .
	
	\hspace{15pt} More over the traditional supervised learning techniques, research has been done to improve prediction in k-NN by using pyramidal decomposition \cite{heroux1998classification},  skip-gram and paragraph vectors-distributed bag of words (PV-DBOW) with multiple discriminant analysis \cite{lauren2018discriminant}, expansion method and Powerset-label mechanism for the short hierarchical classification using the Support Vector Machine (SVM) classifier \cite{salih2018term}, Bayes-optimum decision with the maximum margin principle yields kernels for SVMs \cite{ding2014sensing}.
	
	\section{Proposed Work}
	This projects aims at implementation of various document classification algorithms discussed in Section \ref{model}
	This project also includes stop-words filtering based on stop-words vocabulary list and TF-IDF in attempt to improve performances for each model. The project learns from different datasets; the overview of the datasets is included in later chapters. In addition to discussing algorithm performances, the project also concerns the importance of preserving word ordering within a document with respect to classification accuracy. 
	\subsection{Model and Algorithms}\label{model}
	\begin{itemize}
		\item \textit{ Preprocessing using TF-IDF} \\
		A way to do thresholding aside from just word frequency is by determine how important a word is in determining the class of a document. To determine how important a word is we will be using TF-IDF  weighting, which takes into consideration word frequency as well as document frequency. The weight of the word is the frequency of a word multiplied by the inverse document frequency. A word has the highest weight when with a high word frequency and a low document frequency. Using the TF-IDF, we can set a threshold, which would decrease the feature size and remove irrelevent words \cite{schutze2008introduction}.
		
		\item \textit{Bag-of-words model with Naive Bayes assumption}\\
		The Naive Bayes classifier with bag-of-words model is considering all features as conditionally independent given the class label and evaluated using MPE decision rule.\cite{ishwarspring18} The prediction performance considers to be the baseline for our prediction accuracy. 
		The decision is based on MPE rule:
		\[h_{MPE} = P(Y_j|\pmb{x}_j) \propto P(Y_j)\prod^{d_j}_{i=1}P(w_{i,j}|Y), \;j = 1,\ldots,n\]
		\item Multi-class logistic regression (Maximum Entropy Classifier) \\
		The logistic regression of multiples class is a supervised learning by maximizing the conditional likelihood.
		\[\hat{\theta}(\mathbb{D}) = arg \; max_\theta \sum_{j=1}^{n}p(y_j|x_j,\theta), \;j = 1,\ldots,n\]Without a closed form, best $\theta$ is learnt via gradient decent with respect to the following questions. 
		%\[NLL(\theta) = \sum_{j=1}^{n}ln(\sum_{k=1}^{m}e^{\pmb{w}_k^T\pmb{x}^{ext}_j})-\sum_{k=1}^{m}\pmb{w}_{k}^{T}(\sum_{j=1}^n 1(y_j=k)x^{ext}_{j}\]
		\[\nabla_{\pmb{w}_y}NLL(\theta) = \sum_{j=1}^{n}\bigg(\frac{e^{\pmb{w}_y^T\pmb{x}_j^{ext}}}{\sum_{k=1}^{m}e^{{\pmb{w}_k}^T\pmb{x}_j^{ext}}}-1(y_j = y)\bigg)\pmb{x}^{ext}_{j},\,y=1,\ldots,m.\]
		%\[f(\theta):=NLL(\theta)+\frac{\lambda}{2}\sum_{k=1}^{m}||\pmb{w}{k}||^2_2,\; \lambda>0\]
		\[\nabla_{\pmb{w}_y}f(\theta) = \nabla_{\pmb{w}_y}NLL(\theta)+\lambda\pmb{w}_{y},y=1,\ldots,m.\]
		As a MPE/MAP Classifier, the decision is based on MPE rule where we maximize $f(\theta)$
		\blfootnote{The equations used are from \cite{song1999general} and \cite{ishwarspring18}}
		\item Sensing aware kernel SVM \cite{ding2014sensing}\\
		This kernel design of SVM combines Bayes-optimum decision boundary with Maximum margin principle. \cite{ding2014sensing}
		\item N-gram
		\par The N-grams model uses the probability of each word appearing. For unigram, it considers each word as independent of all other words. Therefore, for a sentence to appear, it would be the probability of each word appearing multiplied together. For bigram, it takes the probability of the first word appearing multiplied by the probability of the second word appearing given the first word and so. The probability of the word depends on the probability of the all the previous words that appear before it in a sentence. \cite{song1999general} \\
		\begin{equation}
		\textit{Unigram:}  \hspace{15pt} P(w_{1,n})  = P(w_1).P(w_2) \cdots P(w_n)
		\end{equation}
		\begin{equation}
		\textit{Bigram:}  \hspace{15pt} P(w_{1,n})  = P(w_1).P(w_2 \mid  w_1) \cdots P(w_n \mid w_{n-1})
		\end{equation}
		In the n-gram models, the features can become very large i.e. $D^N$ for N-gram model which can be reduced by setting a threshold frequency of appearance\cite{furnkranz1998study}.  \\
		

		
	\end{itemize}
	
	\subsection{Code and Dataset}
	We have multiple datasets in which we want to implement these algorithms. 
	\begin{enumerate}
		\item News Group (preprocessed) - 20 class, 11,314 training documents ,  7,532 test documents.
		\item Reuters21578 (preprocessed) - 945 training documents ,  2,347 testing documents \cite{dataset}
		\item Personalized Medicine: Redefining Cancer Treatment - 9 classes,  3,320 training documents , 5,667 testing documents  \cite{kaggledataset}
	\end{enumerate}
		
	\subsection{Minimum Achieviable Plan}
	We would like to implement and compare all the algorithms but if we aren't able to complete all the algorithms we will try to implement atleast Naive Bayes, Logistic Regression, Traditional SVM ,N-gram.
	
	\section{Conclusion}
	In this project, we will be exploring different document classification algorithms. We will be using the Na√Øve Bayes, Logistic Regression, SVM (kernel sensing), and n-grams models. Each of these three algorithms will be tested on three datasets: newsgroup dataset, Reuters21578 dataset and Personalized Medicine: Redefining Cancer Treatment (Kaggle Project) dataset. Each of the algorithms will be taking into account TF-IDF to give each word value of importance. At the end we want to be able to compare the performances of each algorithm in terms of accuracy of classification and computational efficiency across all three datasets. 
	
	\section*{Division of Labor}
	\begin{itemize}
		\item  Arjun Patel
		\begin{itemize}
			\item Logistic Regression
			\item TF-IDF
		\end{itemize}
		\item  Harshil Prajapati
		\begin{itemize}
			\item Pre-Processing of Personalized Medicine: Redefining Cancer Treatment Dataset
			\item N-gram
		\end{itemize}
		\item Bowen Song
		\begin{itemize}
			\item Naive Bayes 
			\item Sensing Aware Kernal SVM
		\end{itemize}
	\end{itemize}
	
	
	\nocite{*}
	{\small
		\bibliographystyle{IEEEtran}
		\bibliography{EC503ref}
	}
\end{document}